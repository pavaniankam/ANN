{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28aa0372-5eb7-4fea-aa1b-3907cc5894b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 2\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 3\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 4\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 5\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 6\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 7\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 8\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 9\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 10\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 11\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 12\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 13\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 14\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 15\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 16\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 17\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 18\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 19\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 20\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 21\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 22\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 23\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 24\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 25\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 26\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 27\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 28\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 29\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 30\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 31\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 32\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 33\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 34\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 35\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 36\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 37\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 38\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 39\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 40\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 41\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 42\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 43\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 44\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 45\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 46\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 47\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 48\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 49\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Epoch: 50\n",
      "Train Accuracy: 100.00%\n",
      "Validation Accuracy: 100.00%\n",
      "\n",
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Define folder paths\n",
    "train_folder = '/home/pavaniankam/Desktop/paathu/lion_train_data/'\n",
    "val_folder = '/home/pavaniankam/Desktop/paathu/lion_validation_data/'\n",
    "test_folder = '/home/pavaniankam/Desktop/paathu/lion_test_data/'\n",
    "# Image size for resizing\n",
    "image_size = (250, 250)\n",
    "\n",
    "# Load training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for filename in os.listdir(train_folder):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        img_path = os.path.join(train_folder, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, image_size)\n",
    "            X_train.append(img.flatten())\n",
    "            y_train.append(1 if filename.startswith('class1') else 0)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Load validation data\n",
    "X_val = []\n",
    "y_val = []\n",
    "for filename in os.listdir(val_folder):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        img_path = os.path.join(val_folder, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, image_size)\n",
    "            X_val.append(img.flatten())\n",
    "            y_val.append(1 if filename.startswith('class1') else 0)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Load test data\n",
    "X_test = []\n",
    "y_test = []\n",
    "for filename in os.listdir(test_folder):\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        img_path = os.path.join(test_folder, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, image_size)\n",
    "            X_test.append(img.flatten())\n",
    "            y_test.append(1 if filename.startswith('class1') else 0)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Initialize weights and bias\n",
    "weights = np.zeros(X_train.shape[1])\n",
    "bias = 0\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "\n",
    "# Training the Perceptron\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    for i in range(X_train.shape[0]):\n",
    "        output = np.dot(X_train[i], weights) + bias\n",
    "        prediction = 1 if output >= 0 else 0\n",
    "        weights += learning_rate * (y_train[i] - prediction) * X_train[i]\n",
    "        bias += learning_rate * (y_train[i] - prediction)\n",
    "    \n",
    "    # Compute training accuracy\n",
    "    correct_train = np.sum(np.where(np.dot(X_train, weights) + bias >= 0, 1, 0) == y_train)\n",
    "    accuracy_train = (correct_train / float(X_train.shape[0])) * 100\n",
    "    \n",
    "    # Compute validation accuracy\n",
    "    correct_val = np.sum(np.where(np.dot(X_val, weights) + bias >= 0, 1, 0) == y_val)\n",
    "    accuracy_val = (correct_val / float(X_val.shape[0])) * 100\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    print(f\"Train Accuracy: {accuracy_train:.2f}%\")\n",
    "    print(f\"Validation Accuracy: {accuracy_val:.2f}%\")\n",
    "    print()\n",
    "\n",
    "# Test the Perceptron on test data\n",
    "correct_test = np.sum(np.where(np.dot(X_test, weights) + bias >= 0, 1, 0) == y_test)\n",
    "accuracy_test = (correct_test / float(X_test.shape[0])) * 100\n",
    "print(f\"Test Accuracy: {accuracy_test:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "480bf7f3-d19d-499e-ad70-93d72bc2e4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8238, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [2/10], Loss: 0.0065, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [3/10], Loss: 0.0002, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [4/10], Loss: 0.0000, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Testing Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "training_path = \"/home/pavaniankam/Desktop/paathu/lion_train_data/\"\n",
    "validation_path = \"/home/pavaniankam/Desktop/paathu/lion_validation_data/\"\n",
    "testing_path = \"/home/pavaniankam/Desktop/paathu/lion_test_data/\"\n",
    "\n",
    "root = training_path\n",
    "transform = transform\n",
    "samples = []\n",
    "for root, _, fnames in sorted(os.walk(root)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            samples.append((path, img))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "\n",
    "train_loader = DataLoader([(transform(sample[1]), 0) for sample in samples], batch_size=32, shuffle=True)\n",
    "\n",
    "root = validation_path\n",
    "transform = transform\n",
    "samples = []\n",
    "for root, _, fnames in sorted(os.walk(root)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            samples.append((path, img))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "\n",
    "val_loader = DataLoader([(transform(sample[1]), 0) for sample in samples], batch_size=32, shuffle=False)\n",
    "\n",
    "root = testing_path\n",
    "transform = transform\n",
    "samples = []\n",
    "for root, _, fnames in sorted(os.walk(root)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            samples.append((path, img))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "\n",
    "test_loader = DataLoader([(transform(sample[1]), 0) for sample in samples], batch_size=32, shuffle=False)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(4):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "test_acc = correct / total\n",
    "print(f\"Testing Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67f82b5-f0cc-4207-b2b4-9b6aac5f1d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.9956, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [2/10], Loss: 0.0592, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [3/10], Loss: 0.0041, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Epoch [4/10], Loss: 0.0007, Train Acc: 1.0000, Val Acc: 1.0000\n",
      "Testing Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "training_path = \"/home/pavaniankam/Desktop/paathu/lion_train_data/\"\n",
    "validation_path = \"/home/pavaniankam/Desktop/paathu/lion_validation_data/\"\n",
    "testing_path = \"/home/pavaniankam/Desktop/paathu/lion_test_data/\"\n",
    "\n",
    "root = training_path\n",
    "transform = transform\n",
    "samples = []\n",
    "for root, _, fnames in sorted(os.walk(root)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            samples.append((path, img))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "\n",
    "train_loader = DataLoader([(transform(sample[1]), 0) for sample in samples], batch_size=32, shuffle=True)\n",
    "\n",
    "root = validation_path\n",
    "transform = transform\n",
    "samples = []\n",
    "for root, _, fnames in sorted(os.walk(root)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            samples.append((path, img))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "\n",
    "val_loader = DataLoader([(transform(sample[1]), 0) for sample in samples], batch_size=32, shuffle=False)\n",
    "\n",
    "root = testing_path\n",
    "transform = transform\n",
    "samples = []\n",
    "for root, _, fnames in sorted(os.walk(root)):\n",
    "    for fname in sorted(fnames):\n",
    "        path = os.path.join(root, fname)\n",
    "        try:\n",
    "            img = Image.open(path).convert('L')\n",
    "            samples.append((path, img))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path}: {str(e)}\")\n",
    "\n",
    "test_loader = DataLoader([(transform(sample[1]), 0) for sample in samples], batch_size=32, shuffle=False)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10),\n",
    "    nn.Linear(10, 10)  # Add this layer\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(4):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "test_acc = correct / total\n",
    "print(f\"Testing Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4149e5a2-56f1-47a1-822c-49a9fceec30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout Regularization:\n",
      "Training Accuracy: 100%\n",
      "Validation Accuracy: 100%\n",
      "Testing Accuracy: 100%\n",
      "\n",
      "L1 Regularization:\n",
      "Training Accuracy: 100%\n",
      "Validation Accuracy: 100%\n",
      "Testing Accuracy: 100%\n",
      "\n",
      "L2 Regularization:\n",
      "Training Accuracy: 100%\n",
      "Validation Accuracy: 100%\n",
      "Testing Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Parameters\n",
    "train_folder_path = '/home/pavaniankam/Desktop/paathu/lion_train_data/'\n",
    "val_folder_path = '/home/pavaniankam/Desktop/paathu/lion_validation_data/'\n",
    "test_folder_path = '/home/pavaniankam/Desktop/paathu/lion_test_data/'\n",
    "\n",
    "# Load and prepare training data\n",
    "files_train = os.listdir(train_folder_path)\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for file_name in files_train:\n",
    "    file_path = os.path.join(train_folder_path, file_name)\n",
    "    img = Image.open(file_path).convert('L')\n",
    "    img = img.resize((64, 64))\n",
    "    arr = np.array(img)\n",
    "    features = arr.flatten() / 255.0\n",
    "    label = 1 if 'class_name' in file_name else 0\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Load and prepare validation data\n",
    "files_val = os.listdir(val_folder_path)\n",
    "X_val = []\n",
    "y_val = []\n",
    "\n",
    "for file_name in files_val:\n",
    "    file_path = os.path.join(val_folder_path, file_name)\n",
    "    img = Image.open(file_path).convert('L')\n",
    "    img = img.resize((64, 64))\n",
    "    arr = np.array(img)\n",
    "    features = arr.flatten() / 255.0\n",
    "    label = 1 if 'class_name' in file_name else 0\n",
    "    X_val.append(features)\n",
    "    y_val.append(label)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Load and prepare test data\n",
    "files_test = os.listdir(test_folder_path)\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for file_name in files_test:\n",
    "    file_path = os.path.join(test_folder_path, file_name)\n",
    "    img = Image.open(file_path).convert('L')\n",
    "    img = img.resize((64, 64))\n",
    "    arr = np.array(img)\n",
    "    features = arr.flatten() / 255.0\n",
    "    label = 1 if 'class_name' in file_name else 0\n",
    "    X_test.append(features)\n",
    "    y_test.append(label)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Shuffle training data\n",
    "shuffle_index = np.random.permutation(len(X_train))\n",
    "X_train = X_train[shuffle_index]\n",
    "y_train = y_train[shuffle_index]\n",
    "\n",
    "# Parameters\n",
    "num_features = X_train.shape[1]\n",
    "learning_rate = 0.1\n",
    "lambda_val = 0.01\n",
    "dropout_rate = 0.5  # Dropout rate (fraction of weights to drop out)\n",
    "epochs = 10\n",
    "\n",
    "# Initialize Perceptron with Dropout Regularization\n",
    "weights_dropout = np.zeros(num_features + 1)  # +1 for the bias\n",
    "\n",
    "# Training with Dropout Regularization\n",
    "for epoch in range(epochs):\n",
    "    for features, label in zip(X_train, y_train):\n",
    "        # Dropout regularization: Create a dropout mask\n",
    "        dropout_mask = np.random.binomial(1, 1 - dropout_rate, size=weights_dropout[1:].shape)\n",
    "        masked_weights_dropout = weights_dropout[1:] * dropout_mask\n",
    "        \n",
    "        activation = np.dot(features, masked_weights_dropout) + weights_dropout[0]\n",
    "        prediction = 1 if activation >= 0 else 0\n",
    "        error = label - prediction\n",
    "        weights_dropout[1:] += learning_rate * error * features\n",
    "        weights_dropout[0] += learning_rate * error\n",
    "\n",
    "# Evaluate the perceptron with Dropout Regularization on training data\n",
    "correct_train_dropout = 0\n",
    "for features, label in zip(X_train, y_train):\n",
    "    dropout_mask = np.random.binomial(1, 1 - dropout_rate, size=weights_dropout[1:].shape)\n",
    "    masked_weights_dropout = weights_dropout[1:] * dropout_mask\n",
    "\n",
    "    activation = np.dot(features, masked_weights_dropout) + weights_dropout[0]\n",
    "    prediction = 1 if activation >= 0 else 0\n",
    "    if prediction == label:\n",
    "        correct_train_dropout += 1\n",
    "train_accuracy_dropout = correct_train_dropout / len(y_train)\n",
    "\n",
    "# Evaluate the perceptron with Dropout Regularization on validation data\n",
    "correct_val_dropout = 0\n",
    "for features, label in zip(X_val, y_val):\n",
    "    dropout_mask = np.random.binomial(1, 1 - dropout_rate, size=weights_dropout[1:].shape)\n",
    "    masked_weights_dropout = weights_dropout[1:] * dropout_mask\n",
    "\n",
    "    activation = np.dot(features, masked_weights_dropout) + weights_dropout[0]\n",
    "    prediction = 1 if activation >= 0 else 0\n",
    "    if prediction == label:\n",
    "        correct_val_dropout += 1\n",
    "val_accuracy_dropout = correct_val_dropout / len(y_val)\n",
    "\n",
    "# Evaluate the perceptron with Dropout Regularization on testing data\n",
    "correct_test_dropout = 0\n",
    "for features, label in zip(X_test, y_test):\n",
    "    dropout_mask = np.random.binomial(1, 1 - dropout_rate, size=weights_dropout[1:].shape)\n",
    "    masked_weights_dropout = weights_dropout[1:] * dropout_mask\n",
    "\n",
    "    activation = np.dot(features, masked_weights_dropout) + weights_dropout[0]\n",
    "    prediction = 1 if activation >= 0 else 0\n",
    "    if prediction == label:\n",
    "        correct_test_dropout += 1\n",
    "test_accuracy_dropout = correct_test_dropout / len(y_test)\n",
    "\n",
    "print(\"Dropout Regularization:\")\n",
    "print(f\"Training Accuracy: {train_accuracy_dropout:.0%}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy_dropout:.0%}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_dropout:.0%}\")\n",
    "\n",
    "# Initialize Perceptron with L1 regularization\n",
    "weights_l1 = np.zeros(num_features + 1)  # +1 for the bias\n",
    "\n",
    "# Training with L1 regularization\n",
    "for epoch in range(epochs):\n",
    "    for features, label in zip(X_train, y_train):\n",
    "        activation = np.dot(features, weights_l1[1:]) + weights_l1[0]\n",
    "        prediction = 1 if activation >= 0 else 0\n",
    "        error = label - prediction\n",
    "        weights_l1[1:] += learning_rate * error * features\n",
    "        weights_l1[0] += learning_rate * error\n",
    "        \n",
    "        # Apply L1 regularization\n",
    "        weights_l1[1:] -= lambda_val * np.sign(weights_l1[1:])\n",
    "\n",
    "# Evaluate the perceptron with L1 regularization on training data\n",
    "correct_train_l1 = 0\n",
    "for features, label in zip(X_train, y_train):\n",
    "    activation = np.dot(features, weights_l1[1:]) + weights_l1[0]\n",
    "    prediction = 1 if activation >= 0 else 0\n",
    "    if prediction == label:\n",
    "        correct_train_l1 += 1\n",
    "train_accuracy_l1 = correct_train_l1 / len(y_train)\n",
    "\n",
    "# Evaluate the perceptron with L1 regularization on validation data\n",
    "correct_val_l1 = 0\n",
    "for features, label in zip(X_val, y_val):\n",
    "    activation = np.dot(features, weights_l1[1:]) + weights_l1[0]\n",
    "    prediction = 1 if activation >= 0 else 0\n",
    "    if prediction == label:\n",
    "        correct_val_l1 += 1\n",
    "val_accuracy_l1 = correct_val_l1 / len(y_val)\n",
    "\n",
    "# Evaluate the perceptron with L1 regularization on testing data\n",
    "correct_test_l1 = 0\n",
    "for features, label in zip(X_test, y_test):\n",
    "    activation = np.dot(features, weights_l1[1:]) + weights_l1[0]\n",
    "    prediction = 1 if activation >= 0 else 0\n",
    "    if prediction == label:\n",
    "        correct_test_l1 += 1\n",
    "test_accuracy_l1 = correct_test_l1 / len(y_test)\n",
    "\n",
    "print(\"\\nL1 Regularization:\")\n",
    "print(f\"Training Accuracy: {train_accuracy_l1:.0%}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy_l1:.0%}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_l1:.0%}\")\n",
    "\n",
    "# Initialize Perceptron with L2 regularization\n",
    "weights_l2 = np.zeros(num_features + 1)  # +1 for the bias\n",
    "\n",
    "# Training with L2 regularization\n",
    "for epoch in range(epochs):\n",
    "    for features, label in zip(X_train, y_train):\n",
    "        activation = np.dot(features, weights_l2[1:]) + weights_l2[0]\n",
    "        prediction = 1 if activation >= 0 else 0\n",
    "        error = label - prediction\n",
    "        weights_l2[1:] += learning_rate * error * features\n",
    "        weights_l2[0] += learning_rate * error\n",
    "        \n",
    "        # Apply L2 regularization\n",
    "        weights_l2[1:] -= lambda_val * weights_l2[1:]\n",
    "\n",
    "# Evaluate the perceptron with L2 regularization on training data\n",
    "correct_train_l2 = 0\n",
    "for features, label in zip(X_train, y_train):\n",
    "    activation = np.dot(features, weights_l2[1:]) + weights_l2[0]\n",
    "    prediction = 1 if activation >= 0 else 0\n",
    "    if prediction == label:\n",
    "        correct_train_l2 += 1\n",
    "train_accuracy_l2 = correct_train_l2 / len(y_train)\n",
    "\n",
    "# Evaluate the perceptron with L2 regularization on validation data\n",
    "correct_val_l2 = 0\n",
    "for features, label in zip(X_val, y_val):\n",
    "    activation = np.dot(features, weights_l2[1:]) + weights_l2[0]\n",
    "    prediction = 1 if activation >= 0 else 0\n",
    "    if prediction == label:\n",
    "        correct_val_l2 += 1\n",
    "val_accuracy_l2 = correct_val_l2 / len(y_val)\n",
    "\n",
    "# Evaluate the perceptron with L2 regularization on testing data\n",
    "correct_test_l2 = 0\n",
    "for features, label in zip(X_test, y_test):\n",
    "    activation = np.dot(features, weights_l2[1:]) + weights_l2[0]\n",
    "    prediction = 1 if activation >= 0 else 0\n",
    "    if prediction == label:\n",
    "        correct_test_l2 += 1\n",
    "test_accuracy_l2 = correct_test_l2 / len(y_test)\n",
    "\n",
    "print(\"\\nL2 Regularization:\")\n",
    "print(f\"Training Accuracy: {train_accuracy_l2:.0%}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy_l2:.0%}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_l2:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55f66475-2209-4308-b67a-effa88cf29a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent (SGD):\n",
      "Training Accuracy: 100%\n",
      "Validation Accuracy: 100%\n",
      "Testing Accuracy: 100%\n",
      "\n",
      "Mini-Batch Gradient Descent:\n",
      "Training Accuracy: 100%\n",
      "Validation Accuracy: 100%\n",
      "Testing Accuracy: 100%\n",
      "\n",
      "Momentum:\n",
      "Training Accuracy: 100%\n",
      "Validation Accuracy: 100%\n",
      "Testing Accuracy: 100%\n",
      "\n",
      "Adam:\n",
      "Training Accuracy: 100%\n",
      "Validation Accuracy: 100%\n",
      "Testing Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Parameters\n",
    "train_folder_path = '/home/pavaniankam/Desktop/paathu/lion_train_data/'\n",
    "val_folder_path = '/home/pavaniankam/Desktop/paathu/lion_validation_data/'\n",
    "test_folder_path = '/home/pavaniankam/Desktop/paathu/lion_test_data/'\n",
    "\n",
    "# Load and prepare data function\n",
    "def load_and_prepare_data(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    X = []\n",
    "    y = []\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        img = Image.open(file_path).convert('L')\n",
    "        img = img.resize((64, 64))\n",
    "        arr = np.array(img)\n",
    "        features = arr.flatten() / 255.0\n",
    "        label = 1 if 'class_name' in file_name else 0\n",
    "        X.append(features)\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load data\n",
    "X_train, y_train = load_and_prepare_data(train_folder_path)\n",
    "X_val, y_val = load_and_prepare_data(val_folder_path)\n",
    "X_test, y_test = load_and_prepare_data(test_folder_path)\n",
    "\n",
    "# Shuffle training data\n",
    "shuffle_index = np.random.permutation(len(X_train))\n",
    "X_train = X_train[shuffle_index]\n",
    "y_train = y_train[shuffle_index]\n",
    "\n",
    "# Parameters\n",
    "num_features = X_train.shape[1]\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "momentum_factor = 0.9\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Helper functions\n",
    "def evaluate(weights, X, y):\n",
    "    correct = 0\n",
    "    for features, label in zip(X, y):\n",
    "        activation = np.dot(features, weights[1:]) + weights[0]\n",
    "        prediction = 1 if activation >= 0 else 0\n",
    "        if prediction == label:\n",
    "            correct += 1\n",
    "    return correct / len(y)\n",
    "\n",
    "# Optimizer: Stochastic Gradient Descent (SGD)\n",
    "weights_sgd = np.zeros(num_features + 1)\n",
    "for epoch in range(epochs):\n",
    "    for features, label in zip(X_train, y_train):\n",
    "        activation = np.dot(features, weights_sgd[1:]) + weights_sgd[0]\n",
    "        prediction = 1 if activation >= 0 else 0\n",
    "        error = label - prediction\n",
    "        weights_sgd[1:] += learning_rate * error * features\n",
    "        weights_sgd[0] += learning_rate * error\n",
    "\n",
    "# Evaluate SGD\n",
    "train_accuracy_sgd = evaluate(weights_sgd, X_train, y_train)\n",
    "val_accuracy_sgd = evaluate(weights_sgd, X_val, y_val)\n",
    "test_accuracy_sgd = evaluate(weights_sgd, X_test, y_test)\n",
    "\n",
    "print(\"Stochastic Gradient Descent (SGD):\")\n",
    "print(f\"Training Accuracy: {train_accuracy_sgd:.0%}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy_sgd:.0%}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_sgd:.0%}\")\n",
    "\n",
    "# Optimizer: Mini-Batch Gradient Descent\n",
    "weights_mbgd = np.zeros(num_features + 1)\n",
    "for epoch in range(epochs):\n",
    "    permutation = np.random.permutation(len(X_train))\n",
    "    X_train_shuffled = X_train[permutation]\n",
    "    y_train_shuffled = y_train[permutation]\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train_shuffled[i:i+batch_size]\n",
    "        batch_y = y_train_shuffled[i:i+batch_size]\n",
    "        for features, label in zip(batch_X, batch_y):\n",
    "            activation = np.dot(features, weights_mbgd[1:]) + weights_mbgd[0]\n",
    "            prediction = 1 if activation >= 0 else 0\n",
    "            error = label - prediction\n",
    "            weights_mbgd[1:] += learning_rate * error * features\n",
    "            weights_mbgd[0] += learning_rate * error\n",
    "\n",
    "# Evaluate Mini-Batch Gradient Descent\n",
    "train_accuracy_mbgd = evaluate(weights_mbgd, X_train, y_train)\n",
    "val_accuracy_mbgd = evaluate(weights_mbgd, X_val, y_val)\n",
    "test_accuracy_mbgd = evaluate(weights_mbgd, X_test, y_test)\n",
    "\n",
    "print(\"\\nMini-Batch Gradient Descent:\")\n",
    "print(f\"Training Accuracy: {train_accuracy_mbgd:.0%}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy_mbgd:.0%}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_mbgd:.0%}\")\n",
    "\n",
    "# Optimizer: Momentum\n",
    "weights_momentum = np.zeros(num_features + 1)\n",
    "velocity = np.zeros(num_features + 1)\n",
    "for epoch in range(epochs):\n",
    "    for features, label in zip(X_train, y_train):\n",
    "        activation = np.dot(features, weights_momentum[1:]) + weights_momentum[0]\n",
    "        prediction = 1 if activation >= 0 else 0\n",
    "        error = label - prediction\n",
    "        gradient = np.concatenate(([error], error * features))\n",
    "        velocity = momentum_factor * velocity + learning_rate * gradient\n",
    "        weights_momentum += velocity\n",
    "\n",
    "# Evaluate Momentum\n",
    "train_accuracy_momentum = evaluate(weights_momentum, X_train, y_train)\n",
    "val_accuracy_momentum = evaluate(weights_momentum, X_val, y_val)\n",
    "test_accuracy_momentum = evaluate(weights_momentum, X_test, y_test)\n",
    "\n",
    "print(\"\\nMomentum:\")\n",
    "print(f\"Training Accuracy: {train_accuracy_momentum:.0%}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy_momentum:.0%}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_momentum:.0%}\")\n",
    "\n",
    "# Optimizer: Adam\n",
    "weights_adam = np.zeros(num_features + 1)\n",
    "m = np.zeros(num_features + 1)\n",
    "v = np.zeros(num_features + 1)\n",
    "for epoch in range(epochs):\n",
    "    for features, label in zip(X_train, y_train):\n",
    "        activation = np.dot(features, weights_adam[1:]) + weights_adam[0]\n",
    "        prediction = 1 if activation >= 0 else 0\n",
    "        error = label - prediction\n",
    "        gradient = np.concatenate(([error], error * features))\n",
    "        m = beta1 * m + (1 - beta1) * gradient\n",
    "        v = beta2 * v + (1 - beta2) * gradient ** 2\n",
    "        m_hat = m / (1 - beta1 ** (epoch + 1))\n",
    "        v_hat = v / (1 - beta2 ** (epoch + 1))\n",
    "        weights_adam += learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "# Evaluate Adam\n",
    "train_accuracy_adam = evaluate(weights_adam, X_train, y_train)\n",
    "val_accuracy_adam = evaluate(weights_adam, X_val, y_val)\n",
    "test_accuracy_adam = evaluate(weights_adam, X_test, y_test)\n",
    "\n",
    "print(\"\\nAdam:\")\n",
    "print(f\"Training Accuracy: {train_accuracy_adam:.0%}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy_adam:.0%}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_adam:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaaa372-36ab-43c6-860c-6e2c106406ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
